{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPq3vAe60R16IYpp//0igtR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bokchisojeong/bokchi_open_lab/blob/main/pytorch_hook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # noqa: N812\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "L3TkiTCkRr5o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_global_watch_idx = 0"
      ],
      "metadata": {
        "id": "f1BQJsyiroQa"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def watch(models,idx=None):\n",
        "  \n",
        "  if not isinstance(models, (tuple, list)):\n",
        "      models = (models,)\n",
        "  global _global_watch_idx\n",
        "  if idx is None:\n",
        "    idx = _global_watch_idx\n",
        "  prefix = \"\"\n",
        "  for local_idx, model in enumerate(models):\n",
        "    global_idx = idx + local_idx\n",
        "    _global_watch_idx += 1\n",
        "    if global_idx > 0:\n",
        "      prefix = \"graph_%i\" % global_idx\n",
        "    print(model)\n",
        "    if not isinstance(model, torch.nn.Module):\n",
        "        raise ValueError(\n",
        "            \"Expected a pytorch model (torch.nn.Module). Received \"\n",
        "            + str(type(model))\n",
        "        )\n",
        "    add_log_gradients_hook(model, name=\"\", prefix=prefix, log_freq=1)\n",
        "      \n",
        "LOG_TRACK_COUNT, LOG_TRACK_THRESHOLD = range(2)\n",
        "\n",
        "\n",
        "def log_track_init(log_freq):\n",
        "    \"\"\"create tracking structure used by log_track_update\"\"\"\n",
        "    l = [0] * 2\n",
        "    l[LOG_TRACK_THRESHOLD] = log_freq\n",
        "    return l\n",
        "def log_track_update(log_track: int) -> bool:\n",
        "    \"\"\"count (log_track[0]) up to threshold (log_track[1]), reset count (log_track[0]) and return true when reached\"\"\"\n",
        "    log_track[LOG_TRACK_COUNT] += 1\n",
        "    if log_track[LOG_TRACK_COUNT] < log_track[LOG_TRACK_THRESHOLD]:\n",
        "        return False\n",
        "    log_track[LOG_TRACK_COUNT] = 0\n",
        "    return True\n",
        "    \n",
        "def add_log_gradients_hook(module, name, prefix,log_freq=0):\n",
        "  prefix = prefix + name\n",
        "  \n",
        "  if not hasattr(model, \"_deepdriver_hook_names\"):\n",
        "    module._deepdriver_hook_names = []\n",
        "      \n",
        "  for name, parameter in model.named_parameters():\n",
        "    print(parameter.requires_grad)\n",
        "    if parameter.requires_grad:\n",
        "        log_track_grad = log_track_init(log_freq)\n",
        "        module._deepdriver_hook_names.append(\"gradients/\" + prefix + name)\n",
        "        _hook_variable_gradient_stats(\n",
        "            parameter, \"gradients/\" + prefix + name, log_track_grad\n",
        "        )\n",
        "def _hook_variable_gradient_stats( var, name, log_track):\n",
        "  \"\"\"Logs a Variable's gradient's distribution statistics next time backward()\n",
        "  is called on it.\n",
        "  \"\"\"\n",
        "\n",
        "  def _callback(grad, log_track):\n",
        "      if not log_track_update(log_track):\n",
        "          return\n",
        "      log_tensor_stats(grad.data, name)\n",
        "\n",
        "  handle = var.register_hook(lambda grad: _callback(grad, log_track))\n",
        "  _hook_handles[name] = handle\n",
        "  return handle\n",
        "def log_tensor_stats( tensor, name):\n",
        "    print(\"log_tensor_stats\")\n",
        "    \"\"\"Add distribution statistics on a tensor's elements to the current History entry\"\"\"\n",
        "    # TODO Handle the case of duplicate names.\n",
        "\n",
        "    if isinstance(tensor, tuple) or isinstance(tensor, list):\n",
        "        while (isinstance(tensor, tuple) or isinstance(tensor, list)) and (\n",
        "            isinstance(tensor[0], tuple) or isinstance(tensor[0], list)\n",
        "        ):\n",
        "            tensor = [item for sublist in tensor for item in sublist]\n",
        "        tensor = torch.cat([t.reshape(-1) for t in tensor])\n",
        "\n",
        "    # checking for inheritance from _TensorBase didn't work for some reason\n",
        "    if not hasattr(tensor, \"shape\"):\n",
        "        cls = type(tensor)\n",
        "        raise TypeError(f\"Expected Tensor, not {cls.__module__}.{cls.__name__}\")\n",
        "\n",
        "    # HalfTensors on cpu do not support view(), upconvert to 32bit\n",
        "    if isinstance(tensor, torch.HalfTensor):\n",
        "        tensor = tensor.clone().type(torch.FloatTensor).detach()\n",
        "\n",
        "    # Sparse tensors have a bunch of implicit zeros. In order to histo them correctly,\n",
        "    # we have to count them up and add them to the histo ourselves.\n",
        "    sparse_zeros = None\n",
        "    if tensor.is_sparse:\n",
        "        # Have to call this on a sparse tensor before most other ops.\n",
        "        tensor = tensor.cpu().coalesce().clone().detach()\n",
        "\n",
        "        backing_values = tensor._values()\n",
        "        non_zero_values = backing_values.numel()\n",
        "        all_values = tensor.numel()\n",
        "        sparse_zeros = all_values - non_zero_values\n",
        "        tensor = backing_values\n",
        "\n",
        "    flat = tensor.reshape(-1)\n",
        "\n",
        "    # For pytorch 0.3 we use unoptimized numpy histograms (detach is new in 0.4)\n",
        "    if not hasattr(flat, \"detach\"):\n",
        "        tensor = flat.cpu().clone().numpy()\n",
        "        wandb.run._log({name: wandb.Histogram(tensor)}, commit=False)\n",
        "        return\n",
        "    print(flat.is_cuda)\n",
        "    print(flat.tolist())\n",
        "    # if flat.is_cuda:\n",
        "    #     # TODO(jhr): see if pytorch will accept something upstream to check cuda support for ops\n",
        "    #     # until then, we are going to have to catch a specific exception to check for histc support.\n",
        "    #     if self._is_cuda_histc_supported is None:\n",
        "    #         self._is_cuda_histc_supported = True\n",
        "    #         check = torch.cuda.FloatTensor(1).fill_(0)\n",
        "    #         try:\n",
        "    #             check = flat.histc(bins=self._num_bins)\n",
        "    #         except RuntimeError as e:\n",
        "    #             # Only work around missing support with specific exception\n",
        "    #             # if str(e).startswith(\"_th_histc is not implemented\"):\n",
        "    #             #    self._is_cuda_histc_supported = False\n",
        "    #             # On second thought, 0.4.1 doesnt have support and maybe there are other issues\n",
        "    #             # lets disable more broadly for now\n",
        "    #             self._is_cuda_histc_supported = False\n",
        "\n",
        "    #     if not self._is_cuda_histc_supported:\n",
        "    #         flat = flat.cpu().clone().detach()\n",
        "\n",
        "    #     # As of torch 1.0.1.post2+nightly, float16 cuda summary ops are not supported (convert to float32)\n",
        "    #     if isinstance(flat, torch.cuda.HalfTensor):\n",
        "    #         flat = flat.clone().type(torch.cuda.FloatTensor).detach()\n",
        "\n",
        "    # if isinstance(flat, torch.HalfTensor):\n",
        "    #     flat = flat.clone().type(torch.FloatTensor).detach()\n",
        "\n",
        "    # # Skip logging if all values are nan or inf or the tensor is empty.\n",
        "    # if self._no_finite_values(flat):\n",
        "    #     return\n",
        "\n",
        "    # # Remove nans and infs if present. There's no good way to represent that in histograms.\n",
        "    # flat = self._remove_infs_nans(flat)\n",
        "\n",
        "    # tmin = flat.min().item()\n",
        "    # tmax = flat.max().item()\n",
        "    # if sparse_zeros:\n",
        "    #     # If we've got zeros to add in, make sure zero is in the hist range.\n",
        "    #     tmin = 0 if tmin > 0 else tmin\n",
        "    #     tmax = 0 if tmax < 0 else tmax\n",
        "    # # Anecdotally, this can somehow happen sometimes. Maybe a precision error\n",
        "    # # in min()/max() above. Swap here to prevent a runtime error.\n",
        "    # if tmin > tmax:\n",
        "    #     tmin, tmax = tmax, tmin\n",
        "    # tensor = flat.histc(bins=self._num_bins, min=tmin, max=tmax)\n",
        "    # tensor = tensor.cpu().clone().detach()\n",
        "    # bins = torch.linspace(tmin, tmax, steps=self._num_bins + 1)\n",
        "\n",
        "    # # Add back zeroes from a sparse tensor.\n",
        "    # if sparse_zeros:\n",
        "    #     bins_np = bins.numpy()\n",
        "    #     tensor_np = tensor.numpy()\n",
        "    #     bin_idx = 0\n",
        "    #     num_buckets = len(bins_np) - 1\n",
        "    #     for i in range(num_buckets):\n",
        "    #         start = bins_np[i]\n",
        "    #         end = bins_np[i + 1]\n",
        "    #         # There are 3 cases to consider here, all of which mean we've found the right bucket\n",
        "    #         # 1. The bucket range contains zero.\n",
        "    #         # 2. The bucket range lower bound *is* zero.\n",
        "    #         # 3. This is the last bucket and the bucket range upper bound is zero.\n",
        "    #         if (start <= 0 and end > 0) or (i == num_buckets - 1 and end == 0):\n",
        "    #             bin_idx = i\n",
        "    #             break\n",
        "\n",
        "    #     tensor_np[bin_idx] += sparse_zeros\n",
        "    #     tensor = torch.Tensor(tensor_np)\n",
        "    #     bins = torch.Tensor(bins_np)\n",
        "\n",
        "    # wandb.run._log(\n",
        "    #     {name: wandb.Histogram(np_histogram=(tensor.tolist(), bins.tolist()))},\n",
        "    #     commit=False,\n",
        "    # )"
      ],
      "metadata": {
        "id": "dhbIUs9bVqPW"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lExy7vJxK4hu",
        "outputId": "e07af482-49f1-4fcd-9a10-1a021daa23d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "NGramLanguageModeler(\n",
            "  (embeddings): Embedding(97, 10, sparse=True)\n",
            "  (linear1): Linear(in_features=20, out_features=128, bias=True)\n",
            "  (linear2): Linear(in_features=128, out_features=97, bias=True)\n",
            ")\n",
            "NGramLanguageModeler(\n",
            "  (embeddings): Embedding(97, 10, sparse=True)\n",
            "  (linear1): Linear(in_features=20, out_features=128, bias=True)\n",
            "  (linear2): Linear(in_features=128, out_features=97, bias=True)\n",
            ")\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# We will use Shakespeare Sonnet 2\n",
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
        "# we should tokenize the input, but we will ignore that for now\n",
        "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
        "trigrams = [\n",
        "    ([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
        "    for i in range(len(test_sentence) - 2)\n",
        "]\n",
        "_hook_handles ={}\n",
        "module=\"torch.nn.Module\"\n",
        "\n",
        "CONTEXT_SIZE = 2\n",
        "EMBEDDING_DIM = 10\n",
        "vocab = set(test_sentence)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "class NGramLanguageModeler(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        print(\"forward\")\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "has_cuda = torch.cuda.is_available()\n",
        "print(has_cuda)\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "model = model.cuda() if has_cuda else model\n",
        "print(model)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "watch(model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y1W2pxY_QZkY"
      },
      "execution_count": 51,
      "outputs": []
    }
  ]
}